{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring-Opportunities-in-the-Indian-Startup-Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Description\n",
    "Our team aims to strategically enter the Indian Startup Ecosystem by leveraging data-driven insights to identify high-potential opportunities. Through comprehensive research and analysis, we seek to gain insight into funding received by startups in India from 2018 to 2021.\n",
    "\n",
    "The project followed the Cross Industry Standard Process for Data Mining (CRISP-DM) framework methodology to solve the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Business Understanding\n",
    "By [THE TIMES OF INDIA](https://timesofindia.indiatimes.com/business/india-business/india-becomes-third-largest-startup-ecosystem-in-the-world/articleshow/85871428.cms), India has emerged as the third largest startup ecosystem in the world after US and China. Following this, our team aims to strategically enter the Indian Startup Ecosystem by leveraging data-driven insights to identify high-potential opportunities. Through comprehensive research and analysis, we seek to gain insight into funding received by startups in India from 2018 to 2021.\n",
    "\n",
    "### 2.1. Hypothesis\n",
    "**Null Hypothesis (Ho):** The sector of a startup has no significant influence on the funding it receives.<br>\n",
    "\n",
    "**Alternative Hypothesis (Ha):** The sector of a startup has significant influence on the funding it receives.\n",
    "\n",
    "### 2.2.  Analytical Questions\n",
    "1. How have funding trends evolved over the years?\n",
    "- Funding trend over the years<br>\n",
    "- Funding evolution across funding stages over the years<br>\n",
    "- Funding trend across the top locations<br>\n",
    "- Funding evolution across top 4 sectors over the years?<br>\n",
    "2. Is there a correlation between the year a startup received funding and the amount of funding it received?<br>\n",
    "3. How does funding vary across the locations of startups' headquarters\n",
    "4. Which sectors can be considered the most attractive?\n",
    "- Determine the top 10 sectors by total funding received<br>\n",
    "- Median funding per sector\n",
    "5. Is there a relationship between funding stage and the amount of funding?\n",
    "- Determine total funding per funding stage\n",
    "- Determine median funding per stage\n",
    "\n",
    "# 3.  Data Understanding\n",
    "This data provides information into amount of money startups received from 2018 to 2021, the sector of startups, headquaters, what a startup do, the year of establishment, startup name, investors, and stage.<br>\n",
    "\n",
    "`Feature Description`:\n",
    "- **Company_Brand:** Name of startup\n",
    "- **Founded:** Year of establishment\n",
    "- **HeadQuater:** Location of startup Headquater\n",
    "- **Sector:** Sector or industry of startup\n",
    "- **What_it_does:** what the startup does\n",
    "- **Founders:** Name od founder\n",
    "- **Investors:** Name of investor\n",
    "- **Amount:** Amount of investment in USD and INR\n",
    "- **Statge:** Phase of development (eg. Ideation Stage, Pre-Seed Stage, Seed Stage, Early Stage (Series A, B, etc.))\n",
    "- **Year:** Year startup received funding\n",
    "\n",
    "### 3.1. Data Collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesary libraries and packages\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values #import the dotenv_values function from the dotenv package\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables and Create SQL Server Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "environment_variables = dotenv_values('.env')\n",
    "# Access login credentials from  the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"UID\")\n",
    "password = environment_variables.get(\"PWD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create Connection connection string\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# connect to the server using pyodbc\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m query2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect * from dbo.LP1_startup_funding2021\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Retrieve dataset from database with connection created\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_2020 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query1, \u001b[43mconnection\u001b[49m)\n\u001b[0;32m      7\u001b[0m df_2021 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query2, connection)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load CSV files\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'connection' is not defined"
     ]
    }
   ],
   "source": [
    "# Write querry to retrieve tables from database\n",
    "query1 = \"Select * from dbo.LP1_startup_funding2020\"\n",
    "query2 = \"Select * from dbo.LP1_startup_funding2021\"\n",
    "\n",
    "# Retrieve dataset from database with connection created\n",
    "df_2020 = pd.read_sql(query1, connection)\n",
    "df_2021 = pd.read_sql(query2, connection)\n",
    "\n",
    "# Load CSV files\n",
    "df_2018 = pd.read_csv('Data\\startup_funding2018.csv')\n",
    "df_2019 = pd.read_csv('Data\\startup_funding2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Issues with the Data\n",
    "- Duplicated rows\n",
    "- Missing values\n",
    "- Inconsistencies in column names and data types\n",
    "- Transposition errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2020 Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataframe\n",
    "df_2020.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check characteristics of dataframe\n",
    "print(df_2020.info(), \"\\n====================== Null Value Percentage ==========================\")\n",
    "# Check for null values\n",
    "print(pd.DataFrame({\"null_value_count\": df_2020.isna().sum(), \"percentage_null_value\": df_2020.isna().mean().mul(100) }), \"\\n====================== Duplicated rows ======================\")\n",
    "# Check for duplicate\n",
    "df_2020.loc[df_2020.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows from DataFrame\n",
    "df_2020.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column10 has 99.8% null values so we drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns10\n",
    "df_2020.drop(columns = [\"column10\"], inplace = True)\n",
    "# Preview remaining columns \n",
    "df_2020.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2021 Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataframe\n",
    "df_2021.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check characteristics of dataframe\n",
    "print(df_2021.info(), \"\\n====================== Null Value Percentage ======================\")\n",
    "# Check for null values\n",
    "print(pd.DataFrame({\"null_value_count\": df_2021.isna().sum(), \"percentage_null_value\": df_2021.isna().mean().mul(100) }), \"\\n====================== Duplicated rows ======================\")\n",
    "# Check for duplicates\n",
    "df_2021.loc[df_2021.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows\n",
    "df_2021.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018 Dataset Exploration and Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataframe\n",
    "df_2018.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check characteristics of dataframe\n",
    "print(df_2018.info(), \"\\n====================== Null Value Percentage ======================\")\n",
    "# Check for null values\n",
    "print(pd.DataFrame({\"null_value_count\": df_2018.isna().sum(), \"percentage_null_value\": df_2018.isna().mean().mul(100) }), \"\\n====================== Duplicated rows ======================\")\n",
    "# Check for duplicates\n",
    "df_2018.loc[df_2018.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows from DataFrame\n",
    "df_2018.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019 Dataset Exploration and Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataframe\n",
    "df_2019.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check characteristics of dataframe\n",
    "print(df_2019.info(), \"\\n====================== Null Value Percentage ======================\")\n",
    "# Check for null values\n",
    "print(pd.DataFrame({\"null_value_count\": df_2019.isna().sum(), \"percentage_null_value\": df_2019.isna().mean().mul(100) }), \"\\n====================== Duplicated rows ======================\")\n",
    "# Check for duplicate\n",
    "df_2019.loc[df_2019.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Handling Identified Issues\n",
    "- Drop duplicates\n",
    "- Filled missing values.\n",
    "    1. fill numerical columns with the median value of that column\n",
    "    2. fill categorical columns with the most frequent values of that column\n",
    "- Standardise column names and data types\n",
    "- Error correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add year_funded column to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year column to dataframe\n",
    "df_2020[\"Year_Funded\"] = 2020\n",
    "df_2021[\"Year_Funded\"] = 2021\n",
    "df_2019[\"Year_Funded\"] = 2019\n",
    "df_2018[\"Year_Funded\"] = 2018\n",
    "\n",
    "# Preview dataframe\n",
    "df_2020.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish uniformity in column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_2019.rename(columns = {'Company/Brand':'Company_Brand','Amount($)':'Amount', 'What it does': 'What_it_does'}, inplace=True)\n",
    "\n",
    "df_2018.rename(columns = {'Company Name':'Company_Brand','Industry':'Sector','Round/Series':'Stage','Location':'HeadQuarter','About Company':'What_it_does'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe and renames the column names to lower case\n",
    "    \"\"\"\n",
    "    df.columns = [col_name.lower() for col_name in df.columns]\n",
    "    return df\n",
    "\n",
    "# Apply function to DataFrames\n",
    "df_2018.pipe(rename_column)\n",
    "df_2019.pipe(rename_column)\n",
    "df_2020.pipe(rename_column)\n",
    "df_2021.pipe(rename_column)\n",
    "\n",
    "# Preview column names\n",
    "df_2021.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if column name is standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare if column names are uniform\n",
    "print(df_2018.columns)\n",
    "print(df_2019.columns)\n",
    "print(df_2020.columns)\n",
    "print(df_2021.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2020' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine all four datasets into a single dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdf_2020\u001b[49m, df_2021, df_2018, df_2019], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Reset index to avoid duplication of index\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df_combined\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_2020' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all four datasets into a single dataframe\n",
    "df_combined = pd.concat([df_2020, df_2021, df_2018, df_2019], axis = 0)\n",
    "# Reset index to avoid duplication of index\n",
    "df_combined.reset_index(drop = True, inplace = True)\n",
    "# Preview combined dataframe\n",
    "df_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check characteristics of dataframe\n",
    "print(df_combined.info(), \"\\n====================== Null Value Count / Percentage ======================\")\n",
    "# Check for null values\n",
    "print(pd.DataFrame({\"null_value_count\": df_combined.isna().sum(), \"percentage_null_value\": df_combined.isna().mean().mul(100) }), \"\\n====================== Duplicated rows ======================\")\n",
    "# Check for duplicated rows\n",
    "df_combined.loc[df_combined.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Column Cleaning\n",
    "### `Column: year_funded`\n",
    "\n",
    "Change datatype of year_funded column from integer to datatime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_to_datetime(column):\n",
    "    \"\"\"\n",
    "    Converts year to datetime format\n",
    "    \"\"\"\n",
    "    # Convert year to datetime format\n",
    "    return pd.to_datetime(column, format = \"%Y\")\n",
    "\n",
    "# Apply function to column\n",
    "df_combined[\"year_funded\"] = df_combined[\"year_funded\"].apply(year_to_datetime)\n",
    "# Confirm comverted datatype \n",
    "df_combined.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview unique entries in the amount column\n",
    "df_combined[\"amount\"].unique()[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount, Stage, and investor columns have some of their values interchanged and need to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview unique entries in the stage column\n",
    "df_combined[\"stage\"].unique()[30:-15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.loc[df_combined[\"stage\"] == \"$1200000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put values in their appropriate columns\n",
    "df_combined.at[1710, \"amount\"], df_combined.at[1710, \"stage\"] = 1000000, np.NAN\n",
    "df_combined.at[1571, \"amount\"], df_combined.at[1571, \"stage\"] = 300000, np.NAN\n",
    "df_combined.at[1584, \"amount\"], df_combined.at[1584, \"stage\"] = 300000, np.NAN\n",
    "df_combined.at[1150, \"amount\"], df_combined.at[1150, \"stage\"] = 1200000, np.NAN\n",
    "df_combined.at[1707, \"amount\"], df_combined.at[1707, \"stage\"] = 6000000, np.NAN\n",
    "df_combined.at[1289, \"amount\"], df_combined.at[1289, \"stage\"], df_combined.at[1289, \"investor\"] = 22000000, \"Series C\", np.NAN\n",
    "df_combined.at[1290, \"amount\"], df_combined.at[1290, \"stage\"], df_combined.at[1290, \"investor\"] = 5000000, \"Seed\", np.NAN\n",
    "df_combined.at[2181, \"amount\"], df_combined.at[2181, \"stage\"], df_combined.at[2181, \"investor\"] = 1000000, \"Seed\", np.NAN\n",
    "df_combined.at[1578, \"amount\"], df_combined.at[1578, \"stage\"], df_combined.at[1578, \"investor\"] = 1000000, \"Pre-series A\", np.NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove currency signs, convert INR (₹) to USD ($), and convert datatype to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Amount colum and convert Indian Rupee to USD currency\n",
    "def clean_amount(value):\n",
    "    \"\"\" \n",
    "    Removes \"$\", and \"₹\"  and converts column to float\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = str(value)\n",
    "        # Remove commas\n",
    "        value = value.replace(\",\", \"\")\n",
    "        # Check if the amount is in INR and convert to USD assuming 1 USD = 70 INR\n",
    "        if \"₹\" in value:\n",
    "            valuet = value.replace(\"₹\", \"\")\n",
    "            return round(float(value) / 70, 2)\n",
    "        # Check if the amount is in USD\n",
    "        elif \"$\" in value:\n",
    "            value = value.replace(\"$\", \"\")\n",
    "            return round(float(value), 2) \n",
    "        # If no currency symbol, assume it's already in USD\n",
    "        else:\n",
    "            return round(float(value), 2)\n",
    "    except ValueError:\n",
    "        # For non-numeric entries, return NaN\n",
    "        return np.NAN\n",
    "\n",
    "# Apply the clean_amount function to the 'amount' column\n",
    "df_combined[\"amount\"] = df_combined[\"amount\"].apply(clean_amount)\n",
    "\n",
    "# Preview dataframe\n",
    "df_combined[\"amount\"].unique()[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: headquarter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview unique values in the headquarter column\n",
    "df_combined[\"headquarter\"].unique()[80:-50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headquarter, sector, and what it does columns have some of their values interchanged and needs to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.loc[df_combined[\"headquarter\"] == \"Food & Beverages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Put values in their appropriate columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf_combined\u001b[49m\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1288\u001b[39m\t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadquarter\u001b[39m\u001b[38;5;124m\"\u001b[39m], df_combined\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1288\u001b[39m\t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHauz Khas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood & Beverages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m df_combined\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1289\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadquarter\u001b[39m\u001b[38;5;124m\"\u001b[39m], df_combined\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1289\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m\"\u001b[39m], df_combined\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1289\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat_it_does\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPharmaceuticals\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m df_combined\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1290\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat_it_does\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_combined\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;241m1290\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "# Put values in their appropriate columns\n",
    "df_combined.at[1288\t, \"headquarter\"], df_combined.at[1288\t, \"sector\"] = \"Hauz Khas\", \"Food & Beverages\"\n",
    "df_combined.at[1289, \"headquarter\"], df_combined.at[1289, \"sector\"], df_combined.at[1289, \"what_it_does\"] = \"None\", \"Pharmaceuticals\", \"None\"\n",
    "df_combined.at[1290, \"what_it_does\"] = df_combined.at[1290, \"sector\"]\n",
    "df_combined.at[1290, \"headquarter\"], df_combined.at[1290, \"sector\"] = \"Gurugram\", \"None\"\n",
    "df_combined.at[2133, \"what_it_does\"] = df_combined.at[2133, \"sector\"]\n",
    "df_combined.at[2133, \"headquarter\"], df_combined.at[2133, \"sector\"] = \"None\", \"Online Media\"\n",
    "df_combined.at[2209, \"headquarter\"], df_combined.at[2209, \"sector\"] = \"Manchester, Greater Manchester\", \"Information Technology & Services\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct spelling mistakes in the headquarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Apply function to column\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m df_combined[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadquarter\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_combined\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadquarter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_column)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_column(value):\n",
    "    \"\"\"\n",
    "    Corrects misspelled values, splits string at the comma, and takes the first string\n",
    "    \"\"\"\n",
    "    replacement = {\"New Delhi\": \"Delhi\", \"New New Delhi\": \"Delhi\", \"San Franciscao\": \"San Francisco\", \"San Francisco Bay Area\": \"San Francisco\",\n",
    "                   \"Bangaldesh\": \"Bangladesh\", \"Milano\": \"Milan\", \"Newcastle Upon Tyne\": \"Newcastle\", \"Hyderebad\": \"Hyderabad\", \"Banglore\": \"Bengaluru\",\n",
    "                   \"Bangalore\": \"Bengaluru\", \"Santra\": \"Santa\", \"Orissa\": \"Odisha\", \"Kormangala\": \"Koramangala\", \"Cochin\": \"Kochi\", \"Orissia\": \"Odisha\",\n",
    "                   \"Thiruvananthapuram\": \"Trivandrum\", \"Samsitpur\": \"Samastipur\", \"Telugana\": \"Telangana\", \"Gurgaon\": \"Gurugram\", \"Rajastan\": \"Rajasthan\",\n",
    "                   \"Uttar pradesh\": \"Uttar Pradesh\", \"Ahmadabad\": \"Ahmedabad\"}\n",
    "    \n",
    "    value = str(value)\n",
    "    # Condition ensures function processes non-None values\n",
    "    if value is not None:\n",
    "        for old_value, new_value in replacement.items():\n",
    "            # Split comma separated values and extract the first value\n",
    "            value = value.split(\",\")[0]\n",
    "            # Replace mispelled values with corrected spelled values\n",
    "            value = value.replace(old_value, new_value)\n",
    "        return value\n",
    "\n",
    "# Apply function to column\n",
    "df_combined[\"headquarter\"] = df_combined[\"headquarter\"].apply(clean_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for entries containing only special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to match strings containing only special characters\n",
    "special_chars = r'^[^\\w\\s]+$'\n",
    "\n",
    "# Filter the DataFrame to include only entries with \"what_it_does\" containing only special characters\n",
    "print(df_combined[df_combined[\"headquarter\"].str.contains(special_chars, na=False)])\n",
    "\n",
    "print(\"\\nThere is no special character present in the column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview headquarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"nan\" with np.NAN for it to be recognised as a null value\n",
    "df_combined[\"headquarter\"].replace(\"nan\", np.NAN, inplace = True)\n",
    "df_combined[\"headquarter\"].replace(\"None\", np.NAN, inplace = True)\n",
    "# Preview column\n",
    "df_combined[\"headquarter\"].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorising headquarter based on location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_names(value):\n",
    "    \"\"\"\n",
    "    Replaces sub cities with their major cities\n",
    "    \"\"\"\n",
    "    cities = {\"Bengaluru\": [\"Bengaluru City\", \"Koramangala\"], \"Delhi\": [\"Kalkaji\", \"Hauz Khas\", \"Azadpur\"], \"California\": [\"San Francisco\", \"Mountain View\", \"San Ramon\", \"Irvine\"],\n",
    "            \"Tamil Nadu\": [\"Chennai\", \"Tirunelveli\", \"Guindy\", \"Mylapore\", \"Kalpakkam\"], \"Maharashtra\": [\"Andheri\", \"Thane\", \"Worli\"],\n",
    "            \"Uttar Pradesh\": [\"Ghaziabad\", \"Uttar\", \"Lucknow\", \"Kanpur\", \"Noida\"], \"Rajasthan\": [\"Alwar\", \"Rajasthan\"], \"Mumbai\": [\"Powai\", \"Andheri\"],\n",
    "            \"Kerala\": [\"Kochi\", \"Alleppey\", \"Kannur\", \"Ernakulam\", \"Trivandrum\"], \"Margão\": [\"Goa\"], \"Anand\": [\"Gujarat\"], \"Hyderabad\": [\"Telangana\"],\n",
    "            \"Bihar\": [\"Samastipur\", \"Bihar\"], \"Rajasthan\": [\"Jodhpur\"], \"Gurgaon\": [\"Gurugram\"]}\n",
    "    \n",
    "    # Put headquarter into categories\n",
    "    for major_city, sub_city in cities.items():\n",
    "        if value in sub_city:\n",
    "            value = major_city\n",
    "    return value\n",
    "\n",
    "# Apply function to column\n",
    "df_combined[\"headquarter\"] = df_combined[\"headquarter\"].apply(city_names)\n",
    "# Preview column\n",
    "df_combined[\"headquarter\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: stage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"stage\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorise Stages of funding according to the Indian Startup Ecosystem [Funding Guide](https://www.startupindia.gov.in/content/sih/en/funding.html) \n",
    "\n",
    "| Stages Classification | Description | Stages of funding per Data given |\n",
    "|----------|-----------|-----------|\n",
    "| Ideation | Brainstorming and developing business concepts, defining value propositions, and outlining plans | Pre-seed, Pre-Seed, Pre seed Round, Early seed, Pre-seed Round, Pre seed round |\n",
    "| Validation | Validating the business model, product-market fit, and scalability through research and feedback | Angel, Seed, Seed A, Seed Funding, Seed round, Seed Round, Seed Round & Series A, Seed Investment, Angel Round, Seed fund, Seed funding, Seed+, Grant, Corporate Round |\n",
    "| Early Traction | Gaining initial traction, attracting early adopters, and refining based on feedback | Series A, Series A-1, Series A+, Series A2, Seies A, Pre-Series A, Pre-series A, Pre series A1, Pre-series A1, Post series A, Pre Series A, Pre series A, Pre-series, Pre- series A, Venture - Series Unknown |\n",
    "| Scaling | Expanding operations, customer base, and market reach for rapid growth | Series B, Series B2, Series B3, Series C, Series C, D, Series D, Series D1, Series E, Series F, Series F1, Series F2, Series G, Series H, Series I, Mid series, Pre-series B, Pre-Series B, Pre series B, Pre-series C, Pre series C, Bridge, Edge, Series B+, Post-IPO Equity, Debt, Debt Financing, Private Equity (PE), PE, Private Equity |\n",
    "| Exit Options | Considering exit strategies such as mergers, acquisitions, or IPOs | Post-IPO Debt, Bridge Round |\n",
    "| Others | Stages with unknown class | Fresh funding, Funding Round, Non-equity Assistance, Secondary Market, Undisclosed, https://docs.google.com/spreadsheets/d/1x9ziNeaz6auNChIHnMI8U6kS7knTr3byy_YBGfQaoUA/edit#gid=1861303593 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group stages according to the Indian Startup Ecosystem funding stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_classification(value):\n",
    "    \"\"\"\n",
    "    Groups stages according to the Indian Startup Ecosystem funding stage\n",
    "    \"\"\"\n",
    "    classification = {\"Ideation\": [\"Pre-seed\", \"Pre-Seed\", \"Pre seed Round\", \"Early seed\", \"Pre-seed Round\", \"Pre seed round\"],\n",
    "                \"Validation\": [\"Angel\", \"Seed\", \"Seed A\", \"Seed Funding\", \"Seed round\", \"Seed Round\", \"Seed Round & Series A\", \"Grant\",\n",
    "                \"Seed Investment\", \"Angel Round\", \"Seed fund\", \"Seed funding\", \"Seed+\",  \"Corporate Round\"],\n",
    "                \"Early Traction\": [\"Series A\", \"Series A-1\", \"Series A+\", \"Series A2\", \"Seies A\", \"Pre-Series A\", \"Pre-series A\", \"Pre- series A\",\n",
    "                \"Venture - Series Unknown\", \"Pre series A1\", \"Pre-series A1\", \"Post series A\", \"Pre Series A\", \"Pre series A\", \"Pre-series\"],\n",
    "                \"Scaling\": [\"Series B+\", \"Series B\", \"Series B2\", \"Series B3\", \"Series C\", \"Series C, D\", \"Series D\", \"Series D1\", \"Series E\",\n",
    "                \"Series F\", \"Series E2\", \"Series F1\", \"Series F2\", \"Series G\", \"Series H\", \"Series I\", \"Mid series\", \"Bridge\", \"Edge\", \"Pre-series C\",\n",
    "                \"Pre series C\", \"Pre-series B\", \"Pre-Series B\", \"Pre series B\", \"Private Equity (PE)\", \"PE\", \"Private Equity\"],\n",
    "                \"Exit Options\": [\"Post-IPO Debt\", \"Post-IPO Equity\", \"Debt\", \"Debt Financing\", \"Bridge Round\"],\n",
    "                \"Others\": [\"Fresh funding\", \"Funding Round\", \"Non-equity Assistance\", \"Secondary Market\", \"Undisclosed\",\n",
    "                           \"https://docs.google.com/spreadsheets/d/1x9ziNeaz6auNChIHnMI8U6kS7knTr3byy_YBGfQaoUA/edit#gid=1861303593\"]}\n",
    "    \n",
    "    # Put stages under class\n",
    "    for classes, stage in classification.items():\n",
    "        if value in stage:\n",
    "            value = classes\n",
    "    return value\n",
    "\n",
    "# Apply function to column\n",
    "df_combined[\"stage\"] = df_combined[\"stage\"].apply(stage_classification)\n",
    "# Preview column\n",
    "df_combined[\"stage\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: sector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview unique entries in the column\n",
    "df_combined[\"sector\"].unique()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sectors column has both single and list of categories separated by commas , indicating the multiple sectors a particular business might be involved in. <br>Extract the sector before the first comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to capture the first value separated by commas\n",
    "regex = r\"^([^,]+)\"\n",
    "\n",
    "# Function to extract first value if it exists, otherwise return the original single-entry value\n",
    "def extract_first_word(value):\n",
    "    # Convert NoneType value to string to avoid errors when iterating over values\n",
    "    value = str(value)\n",
    "    if \",\" in value:\n",
    "        match = re.match(regex, value)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return value\n",
    "\n",
    "# Apply the function to the column\n",
    "df_combined[\"sector\"] = df_combined[\"sector\"].apply(extract_first_word)\n",
    "\n",
    "# Preview unique entries in the sector column\n",
    "df_combined[\"sector\"].unique()[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize industries\n",
    "def categorize_industry(industry):\n",
    "    categories = {\n",
    "        \"Finance sector\": r\"(Accounting|FinTech|Finance|Venture capital|InsureTech|Investment|Financial Services|Banking|Lending|Insurance|Mutual Funds|Capital Markets|Wealth Management|Investment Banking|Online financial service|Escrow)\",\n",
    "        \"Education & Training\": r\"(Provides online learning classes|EdTech|Education|E-learning|Learning|Teaching|Training|School|College|University|Tutoring|Coaching|Classes|Course|Degree|Skills|Edtech|Skill development|EdTech Startup|Education Management|Professional Training|Coaching|Training|Higher Education)\",\n",
    "        \"Agriculture & Environment\": r\"(Tobacco|Fishery|Agri|Agriculture|Farm|Farming|AgTech|Food & Agriculture|Soil-Tech|Environmental|Environment|Sustainability|Sustainable|CleanTech|Green|Pollution|Waste|Recycling|Biomaterials|Environmental Services|Environmental Consulting|Agritech startup|Renewables & Environment|Environmental service|AgriTech)\",\n",
    "        \"Health care\": r\"(Alternative Medicine|Wellness|Healthcare|Dental|Health|HealthTech|Medtech|Biotechnology|Pharmaceuticals|Healthcare|Fertility tech|Fitness|Mental Health|Life sciences|Health Insurance|Pharma|Biopharma|Medical|Health care|Diabetes|Medical Device|Health Diagnostics|Hospital)\",\n",
    "        \"Food & Beverage\": r\"(Dairy startup|Food|Beverages|Restaurant|Dining|Nutrition|Snack Food|Cafe|Brewery|Bar|Craft Beer|Wine|Spirits|Beer|FoodTech|QSR|Restaurant|Food Production|Food & Beverages|Fusion beverages|Food Industry|Food Delivery|Dairy startup|Beverage)\",\n",
    "        \"Retail & E-commerce\": r\"(Appliance|Beauty & wellness|Commercial|Eyeglasses|Merchandise|Social commerce|Commerce|Estore|E store|Retail|E-commerce|Marketplace|Online Portals|Social Commerce|Retail Tech|E-tail|E-market|D2C|Direct-to-consumer|Shopping|Fashion|Clothing|Apparel|Jewellery|Beauty|Cosmetics|E-store|Furniture|Consumer goods|Consumer Electronics|Consumer|Eyewear|Ecommerce|Retail startup|Retail Aggregator|E-Commerce Platforms|D2C Business|D2C startup|E-marketplace|D2C jewellery|E-Commerce|Consumer Goods|Consumer software|Consumer appliances|Consumer Services)\",\n",
    "        \"Energy & utility\": r\"(Energy|Utility|CleanTech|Renewable Energy|Solar|Wind|Hydro|GreenTech|Energy Storage|Oil|Gas|Renewable player|Energy & utility|Power|Fuel|Electricity|Energy|Energy Management|Energy Efficiency|Solar Monitoring Company|Solar solution)\",\n",
    "        \"Logistics & Transportation\": r\"(Logistics|Transportation|Delivery|Shipping|Freight|Supply Chain|Warehousing|Distribution|Cargo|Trucking|Mobility|Fleet|Parcel|Courier|B2B Supply Chain|Transport|Transport Automation|Last Mile Transportation|Transportation|Mobility/Transport|Vehicle|Bike|Car|Automotive|Auto-tech|EV|Electric Vehicle|Logistics & Supply Chain)\",\n",
    "        \"Real Estate & Construction\": r\"(Accomodation|Real Estate|Property|Housing|Construction|Building|Architecture|Infrastructure|Development|Home|Apartment|House|Commercial Real Estate|Residential|Rental|Co-working|Coworking|Co-living|Interior Design|Home Decor|Real estate|Interior|Construction tech|Real Estate & Construction|Property Management|Housing Marketplace|Real Estate)\",\n",
    "        \"Media & Entertainment\": r\"(FM|Blogging|Online storytelling|Basketball|Games|Online Games|Mobile Games|Media|Entertainment|Content|Music|Video|Streaming|TV|Film|Podcast|Broadcasting|Publishing|News|Digital Media|Social Media|Advertising|Marketing|Gaming|Sports|Events|SportsTech|Media and Entertainment|Digital Entertainment|Visual Media|Online Media|Content creation|Content publishing|Celebrity Engagement)\",\n",
    "        \"Consumer Services\": r\"(Legal|Cooking|Advisory firm|Consumer Services|Service|Hospitality|Hotel|Travel|Tourism|Leisure|Customer|Customer service|Professional Services|Consulting|Home services|Lifestyle|Personal care|Dating|Consumer Services|Pet care|Veterinary|Personal Care|Tourism|Hotel|Events|Service industry|Recruitment|Staffing|HR Tech startup|HR Tech|Job portal|Job discovery platform)\",\n",
    "        \"Industrial & Manufacturing\": r\"(Aero company|Aeorspace|Automobile|Aviation|Battery|Tyre management|Industrial|Manufacturing|Engineering|Automotive|Machinery|Equipment|Robotics|Industrial Automation|Aerospace|Defense|Construction|Mining|Chemical|Material|Metallurgy|Heavy Industry|Factory|Production|Assembly|Textiles|Construction tech|Manufacturing|Manufacturing startup|Automotive company|Automobile Technology|Industrial Automation|Mechanical Or Industrial Engineering|Mechanical & Industrial Engineering)\",\n",
    "        \"Communication & Networking\": r\"(Communication|Networking|Telecommunication|Network|Platform|Social Network|Messaging|Collaboration|Media & Networking|Network Security|Telecommuncation|Networking platform|Telecommunications|Networking)\",\n",
    "         \"Technology sector\": r\"(Reading Apps|Internet|OTT|Data Analytics|Data Intelligence|Android|3D Printing|B2B|Crypto|Data Science|IT|Scanning app|IT company|Tech|AI|IoT|SaaS|Software|Cloud|AR|VR|Blockchain|Automation|Robotics|Cybersecurity|Big Data|Machine Learning|Developer Tools|Information Technology|Computer)\"\n",
    "    }\n",
    "    \n",
    "    for category, pattern in categories.items():\n",
    "        if re.search(pattern, industry, re.IGNORECASE):\n",
    "            return category\n",
    "    return industry\n",
    "\n",
    "# Apply function to the column\n",
    "df_combined[\"sector\"] = df_combined[\"sector\"].apply(categorize_industry)\n",
    "# Preview unique entries in the column\n",
    "df_combined[\"sector\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for entries containing only special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to match strings containing only special characters\n",
    "special_chars = r'^[^\\w\\s]+$'\n",
    "\n",
    "# Filter the DataFrame to include only entries with \"what_it_does\" containing only special characters\n",
    "df_combined[df_combined[\"sector\"].str.contains(special_chars, na=False)].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace special characters with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace special characters with null values\n",
    "df_combined[\"sector\"] = df_combined[\"sector\"].replace(\"—\", np.NAN)\n",
    "df_combined[\"sector\"] = df_combined[\"sector\"].replace(\"nan\", np.NAN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: founded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"founded\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data type\n",
    "df_combined[\"founded\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type is float and needs to be converted to integer and when you apply a function that converts values in a column to integers but also assigns np.nan to invalid entries, the resulting column will be of type float64. This is because np.nan is a floating-point representation of \"Not a Number\" (NaN), and a column in a pandas DataFrame that includes np.nan values will be cast to a floating-point type to accommodate these NaN values.<br>\n",
    "\n",
    "**NB**: As a result, we assign 0 to invalid entries for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_integer(value):\n",
    "    try:\n",
    "        # convert the value to an integer\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "# Apply function to coulmn\n",
    "df_combined[\"founded\"] = df_combined[\"founded\"].apply(convert_to_integer)\n",
    "# Confirm datadtype\n",
    "df_combined[\"founded\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: investor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View unique entries in the column\n",
    "df_combined[\"investor\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for entries containing only special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to match strings containing only special characters\n",
    "special_chars = r'^[^\\w\\s]+$'\n",
    "\n",
    "# Filter the DataFrame to include only entries with \"what_it_does\" containing only special characters\n",
    "print(df_combined[df_combined[\"investor\"].str.contains(special_chars, na=False)])\n",
    "\n",
    "print(\"\\n'investor' column has no special character present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[df_combined[\"investor\"] == \"http://100x.vc/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace the hyperlink with a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace hyperlink with null value\n",
    "df_combined[\"investor\"].replace(\"http://100x.vc/\", np.NAN, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Column: what_it_does`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"what_it_does\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for entries containing only special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to match strings containing only special characters\n",
    "special_chars = r'^[^\\w\\s]+$'\n",
    "\n",
    "# Filter the DataFrame to include only entries with \"what_it_does\" containing only special characters\n",
    "print(df_combined[df_combined[\"what_it_does\"].str.contains(special_chars, na=False)])\n",
    "print(\"\\nThere is no special character present in the column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace special characters with Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fill Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Columns: amount, founded`\n",
    "\n",
    "We are going to fill null values in the column. This will be done according to the year startups received funding due to the difference in data characteristics of each dataset.\n",
    "- We filter the datasets by year funded and fill them accordingly.\n",
    "- We fill null values with the median value of each year.<br>\n",
    "\n",
    "**NB**: Mean is affected by outliers and as a result, we use median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because fillna method does not work in-place on a subset of the DataFrame returned by a condition. Instead, we will handle the operation in a step-by-step manner using a function;\n",
    "- First selecting the rows you want to modify\n",
    "- Applying the fillna\n",
    "- Updating the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values in each dataset under the amount column\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2018\"][\"amount\"].isna().sum()} null value(s) in the 2018 dataset under the amount column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2019\"][\"amount\"].isna().sum()} null value(s) in the 2019 dataset under the amount column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2020\"][\"amount\"].isna().sum()} null value(s) in the 2020 dataset under the amount column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2021\"][\"amount\"].isna().sum()} null value(s) in the 2022 dataset under the amount column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the null values in the amount column and reaplace (0)s in the founded column with the median  of each dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_median(df, condition_column, condition_value, target_column):\n",
    "    \"\"\"\n",
    "    Fills NaN values in the target column with the median of that column,\n",
    "    and returns the updated DataFrame with NaN values filled.\n",
    "    \"\"\"\n",
    "    # Select rows that match the condition\n",
    "    subset_df = df[df[condition_column] == condition_value]\n",
    "    # Compute the median of the target column for these rows\n",
    "    median_value = subset_df[target_column].median()\n",
    "    # Fill NaN values in the target column with the median value\n",
    "    subset_df[target_column].fillna(median_value, inplace=True)\n",
    "    # Replace (0) with \"unknown\"\n",
    "    subset_df[target_column].replace(0, median_value, inplace = True)\n",
    "    # Update the original DataFrame with the modified subset\n",
    "    df.update(subset_df)\n",
    "    return df\n",
    "\n",
    "# Apply function to fill null values\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2018\", \"amount\")\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2019\", \"amount\")\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2020\", \"amount\")\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2021\", \"amount\")\n",
    "\n",
    "# Apply function to replace (0)s with median\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2019\", \"founded\")\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2020\", \"founded\")\n",
    "fillna_with_median(df_combined, \"year_funded\", \"2021\", \"founded\")\n",
    "\n",
    "# Check if null values are filled\n",
    "null_value_count_amount = df_combined[\"amount\"].isna().sum()\n",
    "null_value_count_founded = df_combined[\"founded\"].isna().sum()\n",
    "\n",
    "\n",
    "print(f'There are {null_value_count_amount} null value(s) present in the amount column')\n",
    "print(f'There are {null_value_count_founded} (0)s present in the founded column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Column: headquarter`\n",
    "Preview null value counts in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values in each dataset under the headquarter column\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2018\"][\"headquarter\"].isna().sum()} null value(s) in the 2018 dataset under the headquarter column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2019\"][\"headquarter\"].isna().sum()} null value(s) in the 2019 dataset under the headquarter column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2020\"][\"headquarter\"].isna().sum()} null value(s) in the 2020 dataset under the headquarter column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2021\"][\"headquarter\"].isna().sum()} null value(s) in the 2021 dataset under the headquarter column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the null values in the headquarter column with the most frequent value (mode) according to the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_mode(df, condition_column, condition_value, target_column):\n",
    "    \"\"\"\n",
    "    Fills NaN values in the target column with the modal value of that column,\n",
    "    and returns the updated DataFrame with NaN values filled.\n",
    "    \"\"\"\n",
    "    # Select rows that match the condition\n",
    "    sub_set = df[df[condition_column] == condition_value]\n",
    "    # Compute the median of the target column for these rows\n",
    "    mode = sub_set[target_column].mode()[0]\n",
    "    # Fill NaN values in the target column with the modal value\n",
    "    sub_set[target_column].fillna(mode, inplace = True)\n",
    "    # Update the original DataFrame with the modified subset\n",
    "    df.update(sub_set)\n",
    "    return df\n",
    "\n",
    "# Apply function to fill null values\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2019\", \"headquarter\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2020\", \"headquarter\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2021\", \"headquarter\")\n",
    "\n",
    "# Check if null values are filled\n",
    "null_value_count = df_combined[\"headquarter\"].isna().sum()\n",
    "print(f'There are {null_value_count} null value(s) present in the headquarter column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Column: sector`\n",
    "Preview null value counts in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values in each dataset under the sector column\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2018\"][\"sector\"].isna().sum()} null value(s) in the 2018 dataset under the sector column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2019\"][\"sector\"].isna().sum()} null value(s) in the 2019 dataset under the sector column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2020\"][\"sector\"].isna().sum()} null value(s) in the 2020 dataset under the sector column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2021\"][\"sector\"].isna().sum()} null value(s) in the 2021 dataset under the sector column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the null values in the stage column with the most frequent value (mode) according to the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to fill null values\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2018\", \"sector\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2019\", \"sector\")\n",
    "\n",
    "# Check if null values are filled\n",
    "null_value_count = df_combined[\"sector\"].isna().sum()\n",
    "print(f'There are {null_value_count} null value(s) present in the sector column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Column: stage`\n",
    "Preview null value counts in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values in each dataset under the stage column\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2018\"][\"stage\"].isna().sum()} null value(s) in the 2018 dataset under the stage column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2019\"][\"stage\"].isna().sum()} null value(s) in the 2019 dataset under the stage column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2020\"][\"stage\"].isna().sum()} null value(s) in the 2020 dataset under the stage column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2021\"][\"stage\"].isna().sum()} null value(s) in the 2021 dataset under the stage column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the null values in the sector column with the most frequent value (mode) according to the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to fill null values\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2019\", \"stage\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2020\", \"stage\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2021\", \"stage\")\n",
    "\n",
    "# Check if null values are filled\n",
    "null_value_count = df_combined[\"stage\"].isna().sum()\n",
    "print(f'There are {null_value_count} null value(s) present in the stage column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Columns: founders, investor`\n",
    "We won't fill these columns in the 2018 dataset with the modal value because it didn't have these columns. They would rather be filled with **unknown**<br>\n",
    "\n",
    "Preview null values in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values in each dataset under the founders column\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2019\"][\"founders\"].isna().sum()} null value(s) in the 2019 dataset under the founders column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2020\"][\"founders\"].isna().sum()} null value(s) in the 2020 dataset under the founders column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2021\"][\"founders\"].isna().sum()} null value(s) in the 2021 dataset under the founders column')\n",
    "\n",
    "# Check the number of null values in each dataset under the investor column\n",
    "print(f'\\nThere are {df_combined[df_combined[\"year_funded\"] == \"2019\"][\"investor\"].isna().sum()} null value(s) in the 2019 dataset under the investor column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2020\"][\"investor\"].isna().sum()} null value(s) in the 2020 dataset under the investor column')\n",
    "print(f'There are {df_combined[df_combined[\"year_funded\"] == \"2021\"][\"investor\"].isna().sum()} null value(s) in the 2021 dataset under the investor column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the null values in the founders and investor columns with the most frequent value (mode) according to the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to fill null values in the founders column \n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2019\", \"founders\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2020\", \"founders\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2021\", \"founders\")\n",
    "\n",
    "# Apply function to fill null values in the investor column \n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2020\", \"investor\")\n",
    "fillna_with_mode(df_combined, \"year_funded\", \"2021\", \"investor\")\n",
    "\n",
    "# Check if null values are filled\n",
    "null_value_count_founders = df_combined[\"founders\"].isna().sum()\n",
    "null_value_count_investor = df_combined[\"investor\"].isna().sum()\n",
    "\n",
    "print(f\"There are {null_value_count_founders} null value(s) present in the founders column\\nThere are {null_value_count_investor} null value(s) present in the investor column\")\n",
    "\n",
    "# Print statement\n",
    "print(f'\\nNB: The output is giving 525 counts because 2018 has not been filled yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Column: investors, founded`\n",
    "\n",
    "The 2018 Dataset had no founders, investors and founded columns\n",
    "- We fill the categorical columns founders and investors with \"Unknown\".\n",
    "- Leave numerical column founded as NAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_replace_with_unknown(df, condition_column, condition_value, target_column):\n",
    "    \"\"\"\n",
    "    Fills NaN values in the categorical columns with \"Unknown\"\n",
    "    \"\"\"\n",
    "    # Select rows that match the condition\n",
    "    sub_set = df[df[condition_column] == condition_value]\n",
    "    # Fill NaN values in the target column with \"Unknown\"\n",
    "    sub_set[target_column].fillna(\"Unknown\", inplace = True)\n",
    "    # Update the original DataFrame with the modified subset\n",
    "    df.update(sub_set)\n",
    "    return df\n",
    "\n",
    "# Apply function to fill null values\n",
    "fillna_replace_with_unknown(df_combined, \"year_funded\", \"2018\", \"investor\")\n",
    "fillna_replace_with_unknown(df_combined, \"year_funded\", \"2018\", \"founders\")\n",
    "\n",
    "# Check if null values are filled\n",
    "null_value_count_investor = df_combined[\"investor\"].isna().sum()\n",
    "null_value_count_founders = df_combined[\"founders\"].isna().sum()\n",
    "\n",
    "print(f\"There are {null_value_count_investor} null value(s) present in the investor column\\nThere are {null_value_count_founders} null value(s) present in the fouonders column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore data after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview cleaned data set\n",
    "df_combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the characteristics of the cleaned data\n",
    "print(pd.DataFrame({\"null_value_count\": df_combined.isna().sum(), \"datatypes\": df_combined.dtypes}))\n",
    "print(\"\\n============ Data shape ==============\\n\")\n",
    "print(f\"The dataset consists of {df_combined.shape[0]} rows and {df_combined.shape[1]} columns\")\n",
    "print(\"\\n=========== Duplicated rows =================\\n\", df_combined.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview duplicated rows\n",
    "df_combined[df_combined.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows\n",
    "df_combined.drop_duplicates(keep = \"first\", inplace = True)\n",
    "\n",
    "# Check if duplicated rows are dropped\n",
    "print(f\"There are {df_combined.duplicated().sum()} duplicated rows present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistical summary of the dataset\n",
    "df_combined.describe(include = \"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "- Bengaluru is a major hub for startups, accounting for a substantial portion of the dataset. This indicates a geographic concentration of startup activity in Bengaluru.\n",
    "- The technology sector is the most prevalent among the startups, highlighting the dominance of tech-oriented businesses in the startup ecosystem.\n",
    "- The funding amounts vary widely, with a few extremely large values skewing the mean. The majority of startups receive funding in the lower million-dollar range, but there are outliers receiving substantial investments.\n",
    "- A significant number of startups are in the \"Early Traction\" stage, suggesting that many are in the initial phases of growth and development.\n",
    "\n",
    "**NB**: The presence of unknown values in key columns suggests the need for more complete data collection in future analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null Hypothesis (Ho):** The sector of a startup has no significant influence on the funding it receives.<br>\n",
    "**Alternative Hypothesis (Ha):** The sector of a startup has significant influence on the funding it receives.\n",
    "\n",
    "For this hyposthesis testing, we take significant value ($\\alpha$) = 0.05\n",
    "\n",
    "We are going to perform ANOVA analysis to check if the sector of a startup has significant influence on the funding it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA analysis for the factors\n",
    "factors = [\"sector\", \"headquarter\", \"stage\", \"what_it_does\"]\n",
    "\n",
    "model = ols(\"amount ~ C(sector) + C(headquarter) + C(stage) + C(what_it_does)\", data = df_combined).fit()\n",
    "anova_table = sm.stats.anova_lm(model, type = 2)\n",
    "\n",
    "# Define the significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Loop through each factor and its corresponding p-value\n",
    "for factor, p_value in zip(factors, anova_table[\"PR(>F)\"]):\n",
    "    print(f\"Factor: {factor}\")\n",
    "    print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "    if p_value > alpha:\n",
    "        print(f\"Conclusion: Since the p-value is greater than the significance level {alpha}, we fail to reject the null hypothesis.\")\n",
    "        print(f\"This indicates that the {factor} has no significant influence on the funding startups receive.\\n\")\n",
    "    else:\n",
    "        print(f\"Conclusion: Since the p-value is less than the significance level {alpha}, we reject the null hypothesis.\")\n",
    "        print(f\"This indicates that the {factor} has significant influence on the funding startups receive.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our analysis did not find a significant effect of the sector on funding, this does not rule out the possibility that other factors could influence funding. For example, the stage of the startup development showed a significant p-value, suggesting that it might be a more important factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the figure\n",
    "plt.figure(figsize = (10, 5))\n",
    "# Create a violin plot for the \"amount\" column \n",
    "sns.violinplot(df_combined[\"amount\"])\n",
    "# Add a title to the plot\n",
    "plt.title(\"Amount Distribution\")\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot indicates that while a few startups receive substantial funding, most startups receive much smaller amounts.\n",
    "This could reflect an investment strategy where investors are willing to place large bets on a few promising startups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the figure\n",
    "plt.figure(figsize = (10,5))\n",
    "# Create a horizontal bar plot for the top 10 sectors by count\n",
    "df_combined[\"sector\"].value_counts(ascending = False)[:10].plot(kind = \"barh\")\n",
    "# Add title to plot\n",
    "plt.title(\"Top Ten Sectors by Count\")\n",
    "# Lanel x-axis\n",
    "plt.xlabel(\"Total Count of Sectors\")\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize = (10,5))\n",
    "# # Create a horizontal bar plot for the top 10 sectors by total amount received\n",
    "df_combined.groupby(by = \"sector\")[\"amount\"].sum().sort_values(ascending = False)[:10].plot(kind = \"barh\")\n",
    "# Add title to plot\n",
    "plt.title(\"Total Amount Sectors Received\")\n",
    "# Label x-axis\n",
    "plt.xlabel(\"Total Amount (USD in billions)\")\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finance sector: Despite having only 438 counts 2nd highest, the Finance sector received the highest total funding $159.99 bn,\n",
    "indicating a significant average funding per project/company.\n",
    "2. Technology sector: The Technology sector has the highest count 488 but received relatively less funding $8.14 bn\n",
    "compared to Finance and Retail & E-commerce, suggesting a lower average funding per project/company.\n",
    "3. Retail & E-commerce: With the second-highest funding $76.16 bn and a significant count 303,\n",
    "Retail & E-commerce appears to be a rapidly growing sector.\n",
    "4. Education & Training and Logistics & Transportation: These sectors have a consistent performance, with a moderate count 294 and 199 respectively\n",
    "and moderate funding $6.47 bn and $5.27 bn, respectively\n",
    "5. Healthcare and Food & Beverage: These sectors have a stable presence, with a moderate count 274 and 145, respectively\n",
    "and moderate funding $5.04 bn and 4.32 bn, respectively\n",
    "6. Media & Entertainment and Industrial & Manufacturing: These sectors have a relatively lower count 188 and 145 respectively\n",
    "but still received significant funding $4.04 bn and $1.53 bn respectively, indicating a niche presence.\n",
    "7. Multinational conglomerate company: With only a count of 1, this sector received a significant funding $2.2 bn,\n",
    "highlighting the influence of large conglomerates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize = (10, 5))\n",
    "# Create horizontal bar chart of stages by total amount received\n",
    "df_combined.groupby(by = [\"stage\"])[\"amount\"].sum().sort_values(ascending = False).plot(kind = \"barh\")\n",
    "# Add title to plot\n",
    "plt.title(\"Funding stages by Total Amount Received\")\n",
    "# Label x-axis\n",
    "plt.xlabel(\"Total Amount\")\n",
    "# Label y-axis\n",
    "plt.ylabel(\"Funding Stage of Startup\")\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most startups are in the Early Traction (1562) and Validation (743) stages,\n",
    "suggesting that many startups are in the process of proving their concepts and gaining initial traction.\n",
    "\n",
    "The Exit Options stage has the highest total funding ($150.93 billion),\n",
    "indicating significant investments and returns at the point where startups are either going public or being acquired.\n",
    "This is expected since these startups have matured and are exiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Analytical Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. #### How have funding trends evolved over the years?\n",
    "a) Funding trend over ther years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df_combined.groupby(\"year_funded\")[\"amount\"].sum()\n",
    "df_plot = pd.DataFrame(plot).reset_index()\n",
    "\n",
    "# set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot line plot with seaborn\n",
    "plt.figure(figsize = (10, 5))\n",
    "ax = sns.lineplot(data=df_plot, x=\"year_funded\", y=\"amount\", marker=\"o\")\n",
    "\n",
    "plt.title(\"Funding Trend Over the Years\")\n",
    "plt.xlabel(\"Funding Year\")\n",
    "plt.ylabel(\"Total Amount ($ Billion)\")\n",
    "\n",
    "# Configure the x-axis to display major ticks at yearly intervals\n",
    "ax.xaxis.set_major_locator(YearLocator())\n",
    "# Format the y-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "# Adding labels to the points\n",
    "for i, row in df_plot.iterrows():\n",
    "    year = row[\"year_funded\"]\n",
    "    y_value = row[\"amount\"]\n",
    "    label_text = f\"{y_value / 1e9:.1f}B\" if not pd.isna(y_value) else \"N/A\"\n",
    "    plt.text(year, y_value, label_text, ha=\"right\", va=\"bottom\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "\n",
    "- The total funding starts at a relatively low level in 2018 and slight decrease in 2019, indicating either fewer investments or smaller amounts being invested compared to 2018 which could be associated with external factors\n",
    "- The total funding starts to rise noticeably in 2020, suggesting increased investor confidence or more significant funding rounds.<br>\n",
    "- Total Funding increased linearly and peaked in 2021, indicating a substantial influx of capital into startups.<br>\n",
    "This trend suggests growing investor interest and confidence in the startup ecosystem, particularly in the last two years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Funding evolution across funding stages over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for each stage\n",
    "ideation = df_combined[df_combined[\"stage\"] == \"Ideation\"]\n",
    "validation = df_combined[df_combined[\"stage\"] == \"Validation\"]\n",
    "early_traction = df_combined[df_combined[\"stage\"] == \"Early Traction\"]\n",
    "scaling = df_combined[df_combined[\"stage\"] == \"Scaling\"]\n",
    "exit_options = df_combined[df_combined[\"stage\"] == \"Exit Options\"]\n",
    "others = df_combined[df_combined[\"stage\"] == \"Others\"]\n",
    "\n",
    "# Group by year and sum the funding amounts\n",
    "ideation_grouped = ideation.groupby(by = \"year_funded\")[\"amount\"].sum()\n",
    "validation_grouped = validation.groupby(by = \"year_funded\")[\"amount\"].sum()\n",
    "early_traction_grouped = early_traction.groupby(by = \"year_funded\")[\"amount\"].sum()\n",
    "scaling_grouped = scaling.groupby(by = \"year_funded\")[\"amount\"].sum()\n",
    "exit_options_grouped = exit_options.groupby(by = \"year_funded\")[\"amount\"].sum()\n",
    "others_grouped = others.groupby(by = \"year_funded\")[\"amount\"].sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "ideation_grouped.plot(label = \"Ideation\", marker = \"o\")\n",
    "validation_grouped.plot(label = \"Validation\", marker = \"o\")\n",
    "early_traction_grouped.plot(label = \"Early Traction\", marker = \"o\")\n",
    "scaling_grouped.plot(label = \"Scaling\", marker = \"o\")\n",
    "exit_options_grouped.plot(label = \"Exit Options\", marker = \"o\")\n",
    "others_grouped.plot(label = \"Others\", marker = \"o\")\n",
    "\n",
    "# Configure the x-axis to display major ticks at yearly intervals\n",
    "ax.xaxis.set_major_locator(YearLocator())\n",
    "# Format the y-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "plt.title(\"Funding Trends Across Stages Over the Years\")\n",
    "plt.xlabel(\"Funding Year\")\n",
    "plt.ylabel(\"Total Amount\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- There is a visible trend where substantial funding tends to occur in later stages Scaling and Exit Options rather than early stages Ideation,Validation, and Early Traction.\n",
    "- This trend indicates that investors might be more willing to invest larger amounts as ventures progress towards later stages of development and potential exit scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Funding trend across the top locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.groupby(by = \"headquarter\")[\"amount\"].sum().sort_values(ascending = False)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for each sector\n",
    "tech = df_combined[df_combined[\"headquarter\"] == \"Mumbai\"]\n",
    "finance = df_combined[df_combined[\"headquarter\"] == \"Bengaluru\"]\n",
    "retail = df_combined[df_combined[\"headquarter\"] == \"Gurgaon\"]\n",
    "education = df_combined[df_combined[\"headquarter\"] == \"Delhi\"]\n",
    "\n",
    "# Group by year and sum the funding amounts\n",
    "tech_grouped = pd.DataFrame(tech.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "finance_grouped = pd.DataFrame(finance.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "retail_grouped = pd.DataFrame(retail.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "education_grouped = pd.DataFrame(education.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "\n",
    "# Set figure size\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# # Plot each sector with specific colors\n",
    "sns.lineplot(data = tech_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"black\", label = \"Mumbai\")\n",
    "sns.lineplot(data = finance_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"red\", label = \"Bengaluru\")\n",
    "sns.lineplot(data = retail_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"blue\", label = \"Gurgaon\")\n",
    "sns.lineplot(data = education_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"green\", label = \"Delhi\")\n",
    "\n",
    "\n",
    "# Configure the x-axis to display major ticks at yearly intervals\n",
    "ax.xaxis.set_major_locator(YearLocator())\n",
    "# Format the y-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Top 4 Location by Total Funding Across the Years \")\n",
    "plt.xlabel(\"Funding Year\")\n",
    "plt.ylabel(\"Total Amount\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "\n",
    "There is a significant disparity in funding trends, with Mumbai experiencing an exceptional surge in 2021, while other locations like Bengaluru show steady growth and Gurgaon and Delhi have more stable funding patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Funding evoluton across top 4 sectors over the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.groupby(by = \"sector\")[\"amount\"].sum().sort_values(ascending = False)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for each sector\n",
    "tech = df_combined[df_combined[\"sector\"] == \"Technology sector\"]\n",
    "finance = df_combined[df_combined[\"sector\"] == \"Finance sector\"]\n",
    "retail = df_combined[df_combined[\"sector\"] == \"Retail & E-commerce\"]\n",
    "education = df_combined[df_combined[\"sector\"] == \"Education & Training\"]\n",
    "\n",
    "# Group by year and sum the funding amounts\n",
    "tech_grouped = pd.DataFrame(tech.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "finance_grouped = pd.DataFrame(finance.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "retail_grouped = pd.DataFrame(retail.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "education_grouped = pd.DataFrame(education.groupby(by = [\"year_funded\"])['amount'].sum()).reset_index()\n",
    "\n",
    "# Set figure size\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# # Plot each sector with specific colors\n",
    "sns.lineplot(data = tech_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"black\", label = \"Technology\")\n",
    "sns.lineplot(data = finance_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"red\", label = \"Finance\")\n",
    "sns.lineplot(data = retail_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"blue\", label = \"Retail & E-commerce\")\n",
    "sns.lineplot(data = education_grouped, x = \"year_funded\", y = \"amount\", marker = \"o\", color = \"green\", label = \"Education & Training\")\n",
    "\n",
    "\n",
    "# Configure the x-axis to display major ticks at yearly intervals\n",
    "ax.xaxis.set_major_locator(YearLocator())\n",
    "# Format the y-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Top 4 Sectors by Total Funding Across the Years \")\n",
    "plt.xlabel(\"Funding Year\")\n",
    "plt.ylabel(\"Total Amount\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- Finance had a massive surge in funding in 2021, dwarfing the funding amounts for the other sectors.\n",
    "- The retail & e-commerce sector had a significant spike in 2020, possibly due to the pandemic's impact, but then dropped in 2021.\n",
    "- The technology, education & training, and health care sectors have remained relatively stable and low in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Is there a correlation between the year a startup received funding and the amount of funding it received?\n",
    "\n",
    "Plot the amount of funding against the year it was received. Calculate the correlation coefficient to quantify the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression\n",
    "slope, intercept, rvalue, pvalue, stderr = stats.linregress(df_combined[\"year_funded\"].dt.year, df_combined[\"amount\"]/1e11)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot scatterplot\n",
    "sns.scatterplot(x = df_combined[\"year_funded\"].dt.year, y = df_combined[\"amount\"], ax = ax)\n",
    "# Plot regression line\n",
    "plt.plot(df_combined[\"year_funded\"].dt.year, (slope*df_combined[\"year_funded\"].dt.year)+intercept, color = \"black\")\n",
    "\n",
    "# Annotate with the regression equation and statistics\n",
    "plt.annotate(\"Y = %.3fx + %.3f\\nR$^2$ = %.3f\\nP = %.3f\"%(slope, intercept, rvalue**2, pvalue),\n",
    "             xy = (0.1, 0.7),\n",
    "             xycoords = \"figure fraction\")\n",
    "\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Year of Funding Against Total Amount\")\n",
    "plt.xlabel(\"Year of Funding\")\n",
    "plt.ylabel(\"Total Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "- There is no significant relationship between the year of funding and the total amount of funding based on the linear regression model.\n",
    "- P value of 0.37 indicates that the year a startup received funding has no significant influence on the funding amount it received,<br>\n",
    "taking a significant value of 0.05\n",
    "- The significant increases in funding in 2020 and 2021 are likely due to specific external factors rather than a consistent trend over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How does funding vary across the locations of startups' headquarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df_combined.groupby(by = \"headquarter\")[\"amount\"].sum().sort_values(ascending = False)[:10]\n",
    "df_plot = pd.DataFrame(plot)\n",
    "df_plot.reset_index(inplace = True)\n",
    "\n",
    "# Set figure size\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot barplot\n",
    "ax = sns.barplot(data = df_plot, x = \"headquarter\", y = \"amount\")\n",
    "\n",
    "# Add data point to corresponding bars\n",
    "for i, bar in enumerate(plt.gca().patches):\n",
    "    height = bar.get_height()\n",
    "    width = bar.get_width()\n",
    "    x, y = bar.get_xy()\n",
    "    plt.annotate(f\"{int(height)/1e9:.1f}B\", (x + width/2, y + height*1.01), ha = \"center\")\n",
    "\n",
    "# Format the y-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Locations by Total Funding Amount\")\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Total Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "\n",
    "These observations indicate a high concentration of funding in a few major cities, with Mumbai leading by a large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which sectors can be considered the most attractive?\n",
    "\n",
    "a) Determine the top 10 sectors by total funding received\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_sectors = df_combined.groupby(\"sector\")[\"amount\"].sum().sort_values(ascending = False)[:10]\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot barchart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "top_ten_sectors.plot(kind = \"barh\", ax = ax)\n",
    "plt.title(\"Top Ten Sectors by Total Funding Received\")\n",
    "plt.xlabel(\"Sector\")\n",
    "plt.ylabel(\"Amount $\")\n",
    "\n",
    "# Format the x-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "# Annotate the bars\n",
    "for i, (sector, amount) in enumerate(top_ten_sectors.items()):\n",
    "    ax.annotate(f\"{amount / 1e9:.1f}B\", xy=(amount, i), ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "\n",
    "These observations indicate that the Finance sector attracts the most funding, followed by Retail & E-commerce,<br>with a significant drop in funding amounts for the other sectors. This highlights the dominance of these two sectors in attracting investment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Median funding per sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_sectors = df_combined.groupby(\"sector\")[\"amount\"].median().sort_values(ascending = False)[:10]\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot barchart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "top_ten_sectors.plot(kind = \"barh\", ax = ax)\n",
    "plt.title(\"Top Ten Sectors by Median Funding Received\")\n",
    "plt.xlabel(\"Sector\")\n",
    "plt.ylabel(\"Median Amount ($)\")\n",
    "\n",
    "# Annotate the bars\n",
    "for i, (sector, amount) in enumerate(top_ten_sectors.items()):\n",
    "    if round(amount / 1e9, 1) < 1:\n",
    "        ax.annotate(f\"{amount / 1e6:.1f}M\", xy=(amount+145e6, i), ha='right', va='center', fontsize=10)\n",
    "    else:\n",
    "        ax.annotate(f\"{amount / 1e9:.1f}B\", xy=(amount+10e7, i), ha='right', va='center', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Is there a relationship between funding stage and the amount of funding?\n",
    "\n",
    "a) Determine total funding per funding stage? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_stage = df_combined.groupby(\"stage\")[\"amount\"].sum().sort_values(ascending = False)[:10]\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot barchart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "funding_stage.plot(kind = \"barh\", ax = ax)\n",
    "plt.title(\"Total Funding per Stage\")\n",
    "plt.xlabel(\"Stage\")\n",
    "plt.ylabel(\"Total Amount (Billion $)\")\n",
    "\n",
    "# Format the x-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e9)}B\"))\n",
    "\n",
    "# Annotate the bars\n",
    "for i, (stage, amount) in enumerate(funding_stage.items()):\n",
    "    ax.annotate(f\"{amount / 1e9:.1f}B\", xy=(amount, i), ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "\n",
    "Most startups are in the Early Traction 1562 and Validation 743 stages,\n",
    "suggesting that many startups are in the process of proving their concepts and gaining initial traction.\n",
    "\n",
    "The Exit Options stage has the highest total funding $150.90 bn, indicating significant investments and returns at the point where startups are either going public or being acquired.\n",
    "This is expected since these startups have matured and are exiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Determine median funding per stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_stage = df_combined.groupby(\"stage\")[\"amount\"].median().sort_values(ascending = False)[:10]\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot barchart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "funding_stage.plot(kind = \"barh\", ax = ax)\n",
    "plt.title(\"Median Funding per Stage\")\n",
    "plt.xlabel(\"Stage\")\n",
    "plt.ylabel(\"Median Amount (Million ($)\")\n",
    "\n",
    "# Format the x-axis labels to display values in billions of dollars with a 'B' suffix\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x / 1e6)}M\"))\n",
    "\n",
    "# Annotate the bars\n",
    "for i, (stage, amount) in enumerate(funding_stage.items()):\n",
    "    ax.annotate(f\"{amount / 1e6:.1f}M\", xy=(amount, i), ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "\n",
    "From the hypothesis testing, it was concluded with 95% confidence that funding stage has significant influence on the funding startups receive.<br>\n",
    "Also, It could be infered from the plots that startups that are more muture get more funding on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- **Increased Investor Confidence**The total funding for Indian startups has significantly increased, especially in 2020 and peaking in 2021. Investors are showing greater confidence in the potential of Indian start ups, leading to more sustaantial investments.this confidence is built on the historical sucess of previous startups and the promising prospects of newventures.\n",
    "- **Global Economic Recovery** the Global economic recovery following the initial impact of the COVID-19 pandemic has likely played a role. As economies rebound there is more liquidity and willgness to invest in high-growth potential markets like India.\n",
    "-compared to those in early stages like Ideation and Validation. This trend suggests that investors prefer to invest heavily in startups with proven business models and greater growth potential.\n",
    "- **Regional Disparity**: Mumbai stands out with exceptional growth driven by its status as a financial hub, Bengaluru's steady rise backed by its tech ecosystem , Gulgan and Delhi exhibit more stable funding patterns. This suggests that certain locations are more attractive to investors, possibly due to favorable ecosystems or industry concentrations.\n",
    "- **Sectoral Analysis**: Finance experienced a massive surge in funding, overshadowing other sectors significantly. Retail & e-commerce had a spike in 2020, possibly influenced by the pandemic, but experienced a drop in 2021. Technology, education & training, and healthcare sectors remained relatively stable.\n",
    "- **Influence of Funding Stage**: Hypothesis testing confirms that the stage of development significantly influences the funding startups receive. Mature startups attract the highest funding, indicating investor confidence in potential returns.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "**1. Investment Strategy**:\n",
    "- **Growth Stage Investment**: Investment in later stages to help scale successful startups. these investment typically carry less risk and can provide substantial returns as companies expand.\n",
    "- **Geographical Diversification**: Understanding the regional dynamics is crucial for both startups and investors. Mumbai's exceptional growth signals opportunities, while stable patterns in other cities suggest reliability. Investors should focus on top-funded locations like Mumbai, and Bengaluru.\n",
    "- **Sectorial**: stakeholders should focus on the finance sector and also capitalize on diffrent growth areas like technology,healthcare,fintech,agritech and clean energy. \n",
    "\n",
    "**2. Use Of Technology And Data**: Data driven decision making ,use advanced analytics and data to identify trends, assess risks and make informed investment decisions.\n",
    "**Platform for Startups and Investors** Develop platforms that connect startups with potential investors ,offering features like dedal sourcing,due diligence and perfomance tracking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
